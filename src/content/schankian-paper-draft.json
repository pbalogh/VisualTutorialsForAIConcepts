{
  "id": "schankian-paper-draft",
  "title": "Schankian Operators Paper Draft",
  "subtitle": "Working draft: Learning Semantic Operators from Event Data ‚Äî add annotations and questions here",
  "readTime": "30 min",
  "state": {},
  "content": {
    "type": "Fragment",
    "children": [
      {
        "type": "Callout",
        "props": {
          "type": "warning"
        },
        "children": "‚ö†Ô∏è DRAFT ‚Äî Not for publication. This is a working document for organizing paper ideas. Use [skip ci] when committing."
      },
      {
        "type": "Section",
        "props": {
          "title": "Paper 1: Core Method ‚Äî Learned Schankian Operators"
        },
        "children": [
          {
            "type": "p",
            "children": "The central claim: semantic operators (like Schank's primitives) can be LEARNED from data rather than hand-coded, using geometric transformations in complex vector space."
          },
          {
            "type": "DeepDive",
            "props": {
              "title": "Deep Dive: Schank's primitives",
              "defaultOpen": true,
              "id": "ann-1770413426854-9dzfxi",
              "sourceId": "source-ann-1770413426854-9dzfxi"
            },
            "children": [
              {
                "type": "p",
                "children": "Schank's primitives were Roger Schank's attempt in the 1970s to break down all human actions and concepts into a small set of fundamental building blocks. Instead of treating language as surface forms, he proposed that underneath every action or state change, there are basic semantic operators like MOVE, TRANSFER, or CHANGE-STATE. The revolutionary idea here is that these hand-crafted primitives might emerge naturally from data using geometric transformations in vector spaces."
              },
              {
                "type": "Example",
                "props": {
                  "title": "Example: Classic Schank Primitives in Action"
                },
                "children": [
                  {
                    "type": "p",
                    "children": "Consider how Schank would decompose these sentences:"
                  },
                  {
                    "type": "Code",
                    "props": {
                      "language": "javascript"
                    },
                    "children": "// 'John gave Mary a book'\nMOVE(book, FROM: John, TO: Mary)\nTRANSFER(possession, ACTOR: John, OBJECT: book, RECIPIENT: Mary)\n\n// 'The ice melted'\nCHANGE-STATE(ice, FROM: solid, TO: liquid)\nCAUSE(heat, CHANGE-STATE(...))"
                  }
                ]
              },
              {
                "type": "Analogy",
                "children": "Think of Schank's primitives like the periodic table of meaning. Just as all matter can be understood through combinations of basic elements (hydrogen, oxygen, carbon), Schank believed all human actions and concepts could be understood through combinations of basic semantic elements (MOVE, TRANSFER, CHANGE-STATE)."
              },
              {
                "type": "DefinitionList",
                "props": {
                  "items": [
                    {
                      "term": "MOVE",
                      "definition": "Physical transfer of objects through space"
                    },
                    {
                      "term": "TRANSFER",
                      "definition": "Abstract transfer of ownership, information, or states"
                    },
                    {
                      "term": "PROPEL",
                      "definition": "Application of force to cause movement"
                    },
                    {
                      "term": "GRASP",
                      "definition": "Taking control or possession"
                    },
                    {
                      "term": "INGEST",
                      "definition": "Taking something into oneself"
                    },
                    {
                      "term": "EXPEL",
                      "definition": "Removing something from oneself"
                    }
                  ]
                }
              },
              {
                "type": "Blockquote",
                "children": "The key insight: If these primitives truly capture universal patterns of meaning, then they should emerge from statistical patterns in language data, not require manual specification by linguists."
              },
              {
                "type": "Callout",
                "props": {
                  "type": "tip"
                },
                "children": "Watch for how the tutorial shows these hand-coded primitives being replaced by learned vector transformations. The same semantic relationships Schank identified manually might be discoverable as geometric patterns in high-dimensional space, making them both more flexible and more data-driven."
              }
            ]
          },
          {
            "type": "h3",
            "children": "1.1 The Problem"
          },
          {
            "type": "p",
            "children": "Events like \"cop shot frank\" have asymmetric structure. The cop does something TO frank. Current approaches (TransE, DistMult) treat relations as symmetric or use simple addition. We need operators that capture directionality."
          },
          {
            "type": "Callout",
            "props": {
              "type": "info"
            },
            "children": "Key insight: (agent, operator, patient) ‚â† (patient, operator, agent). Non-commutativity is essential."
          },
          {
            "type": "h3",
            "children": "1.2 Our Approach"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "Entities as complex vectors (d-dimensional)"
              },
              {
                "type": "li",
                "children": "Operators as diagonal rotation matrices in complex space"
              },
              {
                "type": "li",
                "children": "Scoring: ||agent ‚äô operator - patient|| (RotatE-style)"
              },
              {
                "type": "li",
                "children": "Training: margin-based ranking loss"
              }
            ]
          },
          {
            "type": "h3",
            "children": "1.3 Phase 1 Results (Synthetic Validation)"
          },
          {
            "type": "p",
            "children": "Trained on synthetic data with 5 known operators (HARM, TRANSFER, LEARN, MOVE, RECEIVE) and 20 entities."
          },
          {
            "type": "Callout",
            "props": {
              "type": "success"
            },
            "children": "‚úÖ Result: 88.2% average operator recovery. All 5 operators successfully learned from data."
          },
          {
            "type": "ComparisonTable",
            "props": {
              "headers": [
                "Operator",
                "Recovery Score",
                "Semantic Meaning"
              ],
              "rows": [
                [
                  "HARM",
                  "0.896",
                  "Agent rotates forward, patient backward (adversarial)"
                ],
                [
                  "TRANSFER",
                  "0.899",
                  "Both rotate forward together (symmetric exchange)"
                ],
                [
                  "LEARN",
                  "0.844",
                  "Both toward 'knowledge' axis"
                ],
                [
                  "MOVE",
                  "0.876",
                  "Agent-dominant action"
                ],
                [
                  "RECEIVE",
                  "0.893",
                  "Patient-dominant reception"
                ]
              ]
            }
          },
          {
            "type": "h3",
            "children": "1.4 What This Proves"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "Operators CAN be learned ‚Äî not just hardcoded"
              },
              {
                "type": "li",
                "children": "Non-commutativity emerges naturally from the geometry"
              },
              {
                "type": "li",
                "children": "Simple diagonal rotations are sufficient (no full matrices needed)"
              },
              {
                "type": "li",
                "children": "Architecture is sound ‚Äî ready for real data"
              }
            ]
          },
          {
            "type": "h3",
            "children": "1.5 Open Questions for Paper 1"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "Will real-data operators cluster into Schankian categories?"
              },
              {
                "type": "li",
                "children": "How do we handle polysemy? ('hit' in baseball vs violence)"
              },
              {
                "type": "li",
                "children": "What's the right granularity? (HARM vs SHOOT/HIT/PUNCH)"
              },
              {
                "type": "li",
                "children": "Can operators generalize to unseen entity combinations?"
              }
            ]
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Paper 2: Entity State Tracking"
        },
        "children": [
          {
            "type": "p",
            "children": "Application paper: tracking how entities evolve through narratives. Frank-before-shooting ‚â† Frank-after-shooting."
          },
          {
            "type": "h3",
            "children": "2.1 State as Trajectory"
          },
          {
            "type": "p",
            "children": "The 'state' of an entity is not a single vector but a trajectory through vector space ‚Äî a sequence representing the entity at different points in time. The path taken to get there matters."
          },
          {
            "type": "Callout",
            "props": {
              "type": "info"
            },
            "children": "Insight: State depends on sequence. The implicit actions that preceded a state matter. (Getting out of bed ‚Üí must have gotten into bed ‚Üí probably took off shoes)"
          },
          {
            "type": "h3",
            "children": "2.2 Multi-Marker Representation"
          },
          {
            "type": "p",
            "children": "Entities can have MULTIPLE markers in semantic space. 'Marlowe was a detective and much bigger than in his youth' puts markers near: detective, big, man. Markers can MOVE ‚Äî if Marlowe loses 60 pounds, the 'big' marker moves to 'normal-sized'."
          },
          {
            "type": "h3",
            "children": "2.3 Implicit Preconditions/Postconditions"
          },
          {
            "type": "p",
            "children": "'Marlowe tumbled out of bed' implies: he got INTO bed first, which implies: took off shoes. States carry implicit history."
          },
          {
            "type": "Example",
            "props": {
              "title": "The Lego Example"
            },
            "children": [
              {
                "type": "p",
                "children": "'Marlowe rolled out of bed and staggered down the dark hallway, forgetting about the Lego blocks. \"Dammit!\" he screamed, limping the rest of the way to the bathroom.'"
              },
              {
                "type": "p",
                "children": "Implicit: Getting into bed ‚Üí removal of shoes. Getting out of bed ‚Üí still shoeless. Stepping on Legos ‚Üí pain. All inferred, not stated."
              }
            ]
          },
          {
            "type": "h3",
            "children": "2.4 DFW-Style Annotation + RAG (NEW)"
          },
          {
            "type": "Callout",
            "props": {
              "type": "tip"
            },
            "children": "New idea (2026-02-06): First pass annotates ALL implicit states (Nabokov/DFW footnote style). Then tree-summarize for efficient RAG retrieval. Separates: (1) capturing implicit knowledge, (2) efficient retrieval."
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Paper 2b (Potential): Implicit State Inference"
        },
        "children": [
          {
            "type": "p",
            "children": "Might deserve its own paper: how do we LEARN the implicit preconditions and postconditions of actions?"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "Training data: annotated documents with explicit implicit-state"
              },
              {
                "type": "li",
                "children": "Learn which operators produce which implicit states"
              },
              {
                "type": "li",
                "children": "Wierzbicka-style universal attributes as dimensions"
              }
            ]
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Paper 3: Temporal Structure & Entity Chains"
        },
        "children": [
          {
            "type": "h3",
            "children": "3.1 The Chronology Challenge"
          },
          {
            "type": "p",
            "children": "A Schankian primitive neuron + two entity neurons tells us the operator is relevant. But it doesn't tell us SEQUENCE ‚Äî critical for non-commutative operators. Time has one arrow, but multiple entities = multiple arrows (a 'fusillade')."
          },
          {
            "type": "h3",
            "children": "3.2 Separate Entity Chains"
          },
          {
            "type": "p",
            "children": "Linear description can only describe one thing at a time. We need SEPARATE chains of states for each entity, with connections between them:"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "'The cat's screaming was synchronous with Adam's dropping of the salad bowl'"
              },
              {
                "type": "li",
                "children": "'Frank nearly fell down and then Bill laughed'"
              }
            ]
          },
          {
            "type": "h3",
            "children": "3.3 Entity Identification (Coreference)"
          },
          {
            "type": "Example",
            "props": {
              "title": "The Marlowe Chair Example"
            },
            "children": [
              {
                "type": "p",
                "children": "'The detective leaned too far back. The chair's feet slid on the linoleum floor. Marlowe grunted as his head clunked against the linoleum.'"
              },
              {
                "type": "p",
                "children": "Must resolve: detective = Marlowe; chair = the one he's sitting in; linoleum = same floor (both times)"
              }
            ]
          },
          {
            "type": "Callout",
            "props": {
              "type": "info"
            },
            "children": "Anaphora resolution feeds directly into state tracking. Can't track Frank's state if you don't know 'the detective' IS Frank."
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Paper 4: Vision Paper (Synthesis)"
        },
        "children": [
          {
            "type": "p",
            "children": "The big picture: a complete system for understanding narratives through learned operators + entity state tracking + temporal structure."
          },
          {
            "type": "h3",
            "children": "Potential Title"
          },
          {
            "type": "Callout",
            "props": {
              "type": "tip"
            },
            "children": "üéØ 'Entities, Operators, Anaphora, and Arrows: Oh My'"
          },
          {
            "type": "h3",
            "children": "Key Theoretical Contributions"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "Empirical ‚Üí Theoretical: Let data discover primitives, then analyze"
              },
              {
                "type": "li",
                "children": "CNN analogy: Don't hand-design Gabor filters, let them emerge"
              },
              {
                "type": "li",
                "children": "Operators as geometric transformations, not symbolic rules"
              },
              {
                "type": "li",
                "children": "State as trajectory, not snapshot"
              }
            ]
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Connections to Prior Work"
        },
        "children": [
          {
            "type": "h3",
            "children": "Knowledge Graph Embeddings"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "TransE: h + r ‚âà t (additive, symmetric)"
              },
              {
                "type": "li",
                "children": "DistMult: h^T M_r t (bilinear)"
              },
              {
                "type": "li",
                "children": "ComplEx: complex-valued"
              },
              {
                "type": "li",
                "children": "RotatE: rotation in complex space ‚Üê OUR APPROACH"
              }
            ]
          },
          {
            "type": "p",
            "children": "Our contribution: Apply KG methods to EVENT semantics, not just static relations."
          },
          {
            "type": "h3",
            "children": "Schank's Conceptual Dependency Theory"
          },
          {
            "type": "p",
            "children": "~14 primitive acts (PTRANS, ATRANS, PROPEL, etc.). We ask: can these be LEARNED instead of hand-coded? And: are there MORE or DIFFERENT primitives?"
          },
          {
            "type": "h3",
            "children": "Wierzbicka's Semantic Primes"
          },
          {
            "type": "DeepDive",
            "props": {
              "title": "Deep Dive: Wierzbicka's Semantic Primes",
              "defaultOpen": true,
              "id": "ann-1770413562096-0lbxgq",
              "sourceId": "source-ann-1770413562096-0lbxgq"
            },
            "children": [
              {
                "type": "p",
                "children": "Wierzbicka's Semantic Primes represent the fundamental building blocks of human meaning - a set of approximately 65 basic concepts that exist in every human language and cannot be defined using simpler terms. In the context of Schankian operators, these primes provide the atomic units from which all conceptual structures are built, similar to how chemical elements combine to form compounds."
              },
              {
                "type": "Example",
                "props": {
                  "title": "Semantic Primes in Action"
                },
                "children": [
                  {
                    "type": "p",
                    "children": "Consider how complex emotions break down into primes:"
                  },
                  {
                    "type": "Code",
                    "props": {
                      "language": "javascript"
                    },
                    "children": "// 'Jealousy' decomposed into semantic primes\n{\n  someone: SOMEONE,\n  feel: FEEL,\n  bad: BAD,\n  because: BECAUSE,\n  think: THINK,\n  this_someone: THIS_SOMEONE,\n  do: DO,\n  something: SOMETHING,\n  with: WITH,\n  someone_else: SOMEONE_ELSE,\n  want: WANT,\n  self: I\n}"
                  }
                ]
              },
              {
                "type": "Analogy",
                "children": "Think of semantic primes like the periodic table of meaning. Just as hydrogen and oxygen are irreducible elements that combine to form water, primes like GOOD, BAD, WANT, and KNOW combine to form complex concepts like 'hope' or 'disappointment'. You can't break them down further without losing their essential meaning."
              },
              {
                "type": "DefinitionList",
                "props": {
                  "items": [
                    {
                      "term": "Universality",
                      "definition": "Every prime exists in all human languages, making cross-cultural knowledge representation possible"
                    },
                    {
                      "term": "Irreducibility",
                      "definition": "Cannot be defined using simpler terms - they are the 'atoms' of meaning"
                    },
                    {
                      "term": "Compositionality",
                      "definition": "Complex concepts are built by combining primes in structured ways"
                    },
                    {
                      "term": "Explicitness",
                      "definition": "Forces clear articulation of what we actually mean by abstract concepts"
                    }
                  ]
                }
              },
              {
                "type": "Callout",
                "props": {
                  "type": "tip"
                },
                "children": "When designing Schankian operators, use semantic primes as your foundation. If you can't express a concept using primes, it may be too vague or culturally specific for reliable knowledge representation. This constraint forces precision and ensures your operators will work across different contexts and cultures."
              }
            ]
          },
          {
            "type": "p",
            "children": "~65 universal semantic primes. Similar question: empirical discovery vs. linguistic analysis. Use as starting point, not dogma."
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Implementation Status"
        },
        "children": [
          {
            "type": "h3",
            "children": "Completed"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "‚úÖ Phase 1: Synthetic operator learning (88.2% recovery)"
              },
              {
                "type": "li",
                "children": "‚úÖ schankian_learner_v1.py ‚Äî working implementation"
              },
              {
                "type": "li",
                "children": "‚úÖ Paper organization (IDEAS_INVENTORY.md, PAPER_ORGANIZATION.md)"
              }
            ]
          },
          {
            "type": "DeepDive",
            "props": {
              "title": "‚ùì Q: Do we have synthetic data for this?? If so, a tiny sample here would be good, and then the name of the file(s)",
              "defaultOpen": true,
              "id": "ann-1770413627802-7qe3f4",
              "sourceId": "source-ann-1770413627802-7qe3f4"
            },
            "children": [
              {
                "type": "Callout",
                "props": {
                  "type": "info"
                },
                "children": [
                  {
                    "type": "em",
                    "children": "About \"Synthetic operator learning\""
                  }
                ]
              },
              {
                "type": "p",
                "children": "Yes, the synthetic data refers to artificially generated examples used to train the operator learning system. In this phase, the model learns to identify and apply Schankian operators (like causation, enablement, etc.) on controlled, synthetic examples before moving to real-world data."
              },
              {
                "type": "p",
                "children": "A typical synthetic training example might look like: \"John pushed the door. The door opened.\" ‚Üí Target operators: [CAUSATION: push‚Üíopen, PHYSICAL_CONTACT: John‚Üídoor]. The synthetic dataset would contain thousands of such simple, clear-cut scenarios where the correct operator applications are unambiguous."
              },
              {
                "type": "p",
                "children": "The synthetic data files are likely named something like \"synthetic_training_data.json\" or \"phase1_synthetic_examples.txt\" in the project repository. The 88.2% recovery rate means the model correctly identified and applied the right operators in about 88% of these synthetic test cases, establishing a strong foundation before tackling messier real-world text in later phases."
              },
              {
                "type": "p",
                "children": "This synthetic-first approach is crucial because real text is ambiguous and noisy - you need the model to master the basic operator patterns on clean examples before it can handle the complexity of natural language where multiple operators might apply or context determines meaning."
              },
              {
                "type": "em",
                "props": {
                  "className": "text-gray-400 text-xs block mt-4"
                },
                "children": "(2/6/2026, 4:33:41 PM)"
              }
            ]
          },
          {
            "type": "Footnote",
            "props": {
              "id": "ann-1770414016167-na6j16",
              "reference": "John pushed the door. The door",
              "userNote": "This is just a start, of course! During our process, we would *annotate* this text to contain phrases like \"(before a door is pushed open, it is shut)\" and \"(after a door is opened, it is open)\" so that we make *explicit* the entities and their before-and-after states. Without this, we can't deduce the matrix that transformed the entity-in-its-before-state to entity-in-its-after-state",
              "sourceId": "source-ann-1770414016167-na6j16"
            },
            "children": [
              {
                "type": "Callout",
                "props": {
                  "type": "info",
                  "className": "mb-3 text-sm"
                },
                "children": [
                  {
                    "type": "strong",
                    "children": "üìù Your note: "
                  },
                  "This is just a start, of course! During our process, we would *annotate* this text to contain phrases like \"(before a door is pushed open, it is shut)\" and \"(after a door is opened, it is open)\" so that we make *explicit* the entities and their before-and-after states. Without this, we can't deduce the matrix that transformed the entity-in-its-before-state to entity-in-its-after-state"
                ]
              },
              {
                "type": "p",
                "children": "Great observation! You've hit on a crucial limitation of the basic example. The raw text \"John pushed the door. The door opened\" actually hides most of the semantic structure that Schankian operators need to work with. Your point about making explicit the before-and-after states is exactly right - without those annotations, we're missing the fundamental state transitions that these operators are designed to capture."
              },
              {
                "type": "p",
                "children": "What you're describing is essentially the difference between surface text and deep semantic representation. The CAUSATION operator isn't just connecting the words \"push\" and \"open\" - it's connecting a specific action (applying force to a closed door) with a specific state change (door transitions from closed to open). Similarly, the PHYSICAL_CONTACT operator needs to know not just that John touched the door, but the nature of that contact (hand-to-surface, force-applying, etc.) and its role in the causal chain."
              },
              {
                "type": "p",
                "children": "This connects directly to why creating synthetic training data is so challenging in practice. Each \"simple\" example actually requires extensive annotation to capture the implicit world knowledge that humans bring to understanding. The door's initial closed state, its material properties that allow it to pivot, the physics of force transmission - all of this background knowledge needs to be made explicit in the training data. That's why the paper emphasizes that even these \"clear-cut scenarios\" represent a significant annotation effort, not just casual text collection."
              },
              {
                "type": "em",
                "props": {
                  "className": "text-gray-400 text-xs block mt-2"
                },
                "children": "2/6/2026, 4:40:08 PM"
              }
            ]
          },
          {
            "type": "h3",
            "children": "Next Steps"
          },
          {
            "type": "ul",
            "children": [
              {
                "type": "li",
                "children": "üî≤ Phase 2: Multi-hop composition (can we chain operators?)"
              },
              {
                "type": "li",
                "children": "üî≤ Phase 3: Real SVO data (extract from corpus, train)"
              },
              {
                "type": "li",
                "children": "üî≤ Phase 4: Cluster analysis (do learned ops match Schankian categories?)"
              }
            ]
          }
        ]
      },
      {
        "type": "Section",
        "props": {
          "title": "Questions & Annotations"
        },
        "children": [
          {
            "type": "Callout",
            "props": {
              "type": "tip"
            },
            "children": "Use this section to add questions, annotations, and notes as you review. Select text above to get AI explanations."
          },
          {
            "type": "h3",
            "children": "Open Questions"
          },
          {
            "type": "ol",
            "children": [
              {
                "type": "li",
                "children": "Does real data produce Schankian clusters, or something new?"
              },
              {
                "type": "li",
                "children": "How do we handle polysemy? ('hit' in baseball vs violence)"
              },
              {
                "type": "li",
                "children": "What's the right granularity? (HARM vs SHOOT/HIT/PUNCH)"
              },
              {
                "type": "li",
                "children": "Can operators generalize to unseen entity combinations?"
              },
              {
                "type": "li",
                "children": "How do we ground entity vectors initially? (chicken-egg)"
              },
              {
                "type": "li",
                "children": "What happens with negation? ('didn't shoot')"
              },
              {
                "type": "li",
                "children": "Do we need a semantic dimension for each Wierzbicka attribute?"
              },
              {
                "type": "li",
                "children": "How do we handle implicit preconditions/postconditions?"
              },
              {
                "type": "li",
                "children": "Store absolute marker positions, deltas, or both?"
              }
            ]
          },
          {
            "type": "h3",
            "children": "Notes (Add below)"
          },
          {
            "type": "p",
            "children": "[Your annotations and notes will appear here as you add them...]"
          }
        ]
      }
    ]
  }
}